{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading cables data\n",
    "\n",
    "Desired output `Vector{Cable}` where `Cable` contains `time::Float64`, `node::Int`, and `mark::SparseVector{Int}`.\n",
    "\n",
    "Some notes on the data:\n",
    "\n",
    "- `train.tsv`, `test.csv`, and `validation.tsv` contain the train, test, and validation sets, respectively. Each row in the file contains three (tab-delimited) integers: the document ID, the word ID, and the number of words.\n",
    "- `meta.tsv` contains the nodes and time IDs of each the documents. Each row in the file contains three (tab-delimited) integers: the document ID, the node ID, and the timestamp ID.\n",
    "\n",
    "We'll use the following universal column names when working with tables:\n",
    "- `docid`: a unique document ID\n",
    "- `nodeid`: a unique node ID\n",
    "- `dateid`: a unique timestamp ID (note: this is to be contrasted with the actual time itself, which has a meaninful numerical value)\n",
    "\n",
    "- `word_count`: the number of times `wordid` appears in `docid`\n",
    "- `date`: the actual date in a string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CSV\n",
    "import JLD\n",
    "using DataFrames\n",
    "using SparseArrays\n",
    "\n",
    "DATADIR = \"/home/anthony/data/cables/\"\n",
    "\n",
    "UNIQUE_ENTITIES = \"unique_entities.txt\"\n",
    "UNIQUE_DATES = \"unique_dates.txt\"\n",
    "\n",
    "META = \"meta.tsv\"\n",
    "DATES = \"dates.tsv\"\n",
    "\n",
    "TRAIN = \"train.tsv\"\n",
    "TEST = \"test.tsv\"\n",
    "VALIDATION = \"validation.tsv\"\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first(meta, 5) = 5×3 DataFrame\n",
      "│ Row │ docid │ nodeid │ date  │\n",
      "│     │ Int64 │ Int64  │ Int64 │\n",
      "├─────┼───────┼────────┼───────┤\n",
      "│ 1   │ 0     │ 0      │ 2002  │\n",
      "│ 2   │ 1     │ 1      │ 181   │\n",
      "│ 3   │ 2     │ 1      │ 182   │\n",
      "│ 4   │ 3     │ 1      │ 183   │\n",
      "│ 5   │ 4     │ 1      │ 183   │\n"
     ]
    }
   ],
   "source": [
    "dataset_name = TRAIN\n",
    "\n",
    "# Load the metadata\n",
    "meta = DataFrame(CSV.File(DATADIR * META, header=[:docid, :nodeid, :date]))\n",
    "\n",
    "@show first(meta, 5)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sparse doc-word matrix\n",
    "doc_inds = Int[]\n",
    "word_inds = Int[]\n",
    "word_counts = Int[]\n",
    "\n",
    "# Instead of using a DataFrame, we'll have\n",
    "# to do this in place because of the size of\n",
    "# the train dataset.\n",
    "for row in CSV.Rows(\n",
    "    DATADIR * dataset_name, \n",
    "    header=[:docid, :wordid, :word_count],\n",
    "    types=[Int, Int, Int]\n",
    ")\n",
    "    push!(doc_inds, row.docid+1)\n",
    "    push!(word_inds, row.wordid+1)\n",
    "    push!(word_counts, row.word_count)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2114195×21819 SparseMatrixCSC{Int64,Int64} with 112674036 stored entries:\n",
       "  [31     ,     1]  =  1\n",
       "  [55     ,     1]  =  1\n",
       "  [62     ,     1]  =  1\n",
       "  [65     ,     1]  =  1\n",
       "  [67     ,     1]  =  1\n",
       "  [73     ,     1]  =  1\n",
       "  [75     ,     1]  =  2\n",
       "  [92     ,     1]  =  1\n",
       "  [148    ,     1]  =  2\n",
       "  [161    ,     1]  =  1\n",
       "  [166    ,     1]  =  1\n",
       "  [238    ,     1]  =  1\n",
       "  ⋮\n",
       "  [1915747, 21819]  =  1\n",
       "  [1917058, 21819]  =  1\n",
       "  [1918733, 21819]  =  1\n",
       "  [1918878, 21819]  =  1\n",
       "  [1918943, 21819]  =  1\n",
       "  [1921051, 21819]  =  1\n",
       "  [1921129, 21819]  =  1\n",
       "  [1964356, 21819]  =  1\n",
       "  [1981318, 21819]  =  1\n",
       "  [2034855, 21819]  =  1\n",
       "  [2035548, 21819]  =  1\n",
       "  [2055796, 21819]  =  1\n",
       "  [2071179, 21819]  =  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill the matrix with each value\n",
    "doc_word_mat = sparse(doc_inds, word_inds, word_counts)\n",
    "\n",
    "# Remove garbage from memory (please don't die computer!)\n",
    "word_inds = nothing\n",
    "word_counts = nothing\n",
    "GC.gc()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the unique document IDs in the dataset\n",
    "docs = unique(doc_inds)\n",
    "\n",
    "# Get the node for each document\n",
    "nodes = meta[docs, :nodeid]\n",
    "\n",
    "# Finally, get the dates for each document\n",
    "dates = meta[docs, :date]\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result as a JLD file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = DATADIR * dataset_name[1 : end-4] * \".jld\"\n",
    "JLD.@save filepath docs dates nodes doc_word_mat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
